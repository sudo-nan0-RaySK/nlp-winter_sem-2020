{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnum Ops 2 (Cosine Similarity - Text Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saksham Sethi, 17BCE0754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import*\n",
    "line = ''\n",
    "wordlist = list()\n",
    "flist= glob.glob(\"Folders/*.txt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Folders/two.txt', 'Folders/four.txt', 'Folders/three.txt', 'Folders/one.txt']\n"
     ]
    }
   ],
   "source": [
    "print(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we are, in the world of wonders.\n",
      "Hello, are you feeling alright.\n",
      "\n",
      "\n",
      "Hello, This is my life.\n",
      "Something to do with me, something to do with my dog.\n",
      "\n",
      "Hello, I have a dog.\n",
      "The dog name is paul.\n",
      "He is a good boy.\n",
      "\n",
      "Hello my name is adit.\n",
      "Hello, How are you?\n",
      "What is your name?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reading text docs and storing them\n",
    "line = []; i = 0\n",
    "for fname in flist:\n",
    "    tfile = open(fname,\"r\")\n",
    "    line.append(tfile.read())\n",
    "    print(line[i])\n",
    "    i+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'we', 'are', ',', 'in', 'the', 'world', 'of', 'wonders', '.', 'Hello', ',', 'are', 'you', 'feeling', 'alright', '.'], ['Hello', ',', 'This', 'is', 'my', 'life', '.', 'Something', 'to', 'do', 'with', 'me', ',', 'something', 'to', 'do', 'with', 'my', 'dog', '.'], ['Hello', ',', 'I', 'have', 'a', 'dog', '.', 'The', 'dog', 'name', 'is', 'paul', '.', 'He', 'is', 'a', 'good', 'boy', '.'], ['Hello', 'my', 'name', 'is', 'adit', '.', 'Hello', ',', 'How', 'are', 'you', '?', 'What', 'is', 'your', 'name', '?']]\n"
     ]
    }
   ],
   "source": [
    "##Tokenzing the documents\n",
    "tokenize = []\n",
    "for i in range(0,len(line)):\n",
    "    tokenize.append(word_tokenize(line[i]))\n",
    "print(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['world', 'wonders', 'hello', 'feeling', 'alright'], ['hello', 'life', 'something', 'something', 'dog'], ['hello', 'dog', 'dog', 'name', 'paul', 'good', 'boy'], ['hello', 'name', 'adit', 'hello', 'name']]\n"
     ]
    }
   ],
   "source": [
    "##Removing Stopwords\n",
    "tokenize2=[]\n",
    "for i in range(0,len(tokenize)):\n",
    "    token2 = []\n",
    "    for j in tokenize[i]:\n",
    "        if(j.lower() not in stopwords.words(\"english\") and j not in string.punctuation):\n",
    "            token2.append(j.lower())\n",
    "    tokenize2.append(token2)\n",
    "print(tokenize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['world', 'wonders', 'hello', 'feeling', 'alright', 'wond', 'feel'], ['hello', 'life', 'something', 'something', 'dog', 'lif', 'someth', 'someth', 'some', 'some', 'som', 'som'], ['hello', 'dog', 'dog', 'name', 'paul', 'good', 'boy', 'nam'], ['hello', 'name', 'adit', 'hello', 'name', 'nam', 'nam']]\n"
     ]
    }
   ],
   "source": [
    "##Stemming the words\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "for i in range(0,len(tokenize2)):\n",
    "    for j in tokenize2[i]:\n",
    "        if(ls.stem(j)!=j):\n",
    "            tokenize2[i].append(ls.stem(j))\n",
    "print(tokenize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'world': 1, 'wonders': 1, 'hello': 1, 'feeling': 1, 'alright': 1, 'wond': 1, 'feel': 1}, {'hello': 1, 'life': 1, 'something': 2, 'dog': 1, 'lif': 1, 'someth': 2, 'some': 2, 'som': 2}, {'hello': 1, 'dog': 2, 'name': 1, 'paul': 1, 'good': 1, 'boy': 1, 'nam': 1}, {'hello': 2, 'name': 2, 'adit': 1, 'nam': 2}]\n"
     ]
    }
   ],
   "source": [
    "## Making the dictionary out of the words\n",
    "tokenize3=[]\n",
    "for k in range(0,len(tokenize2)): \n",
    "    token3 = {}\n",
    "    for i in tokenize2[k]:\n",
    "        if(i not in token3.keys()):\n",
    "            token3[i]=1\n",
    "        else:\n",
    "            token3[i]=token3[i]+1\n",
    "    tokenize3.append(token3)\n",
    "print(tokenize3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adit', 'alright', 'boy', 'dog', 'feel', 'feeling', 'good', 'hello', 'lif', 'life', 'nam', 'name', 'paul', 'som', 'some', 'someth', 'something', 'wond', 'wonders', 'world']\n",
      "[['world', 'wonders', 'hello', 'feeling', 'alright', 'wond', 'feel'], ['hello', 'life', 'something', 'something', 'dog', 'lif', 'someth', 'someth', 'some', 'some', 'som', 'som'], ['hello', 'dog', 'dog', 'name', 'paul', 'good', 'boy', 'nam'], ['hello', 'name', 'adit', 'hello', 'name', 'nam', 'nam']]\n"
     ]
    }
   ],
   "source": [
    "## Making a set of all unique terms\n",
    "s = set()\n",
    "for i in range(0,len(tokenize2)):\n",
    "    st = ()\n",
    "    st = \" \".join(tokenize2[i])\n",
    "    s =  s.union(set(st.split(' ')))\n",
    "s = sorted(s)\n",
    "print(s)\n",
    "print(tokenize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adit [0, 0, 0, 1]\n",
      "alright [1, 0, 0, 0]\n",
      "boy [0, 0, 1, 0]\n",
      "dog [0, 1, 2, 0]\n",
      "feel [2, 0, 0, 0]\n",
      "feeling [1, 0, 0, 0]\n",
      "good [0, 0, 1, 0]\n",
      "hello [1, 1, 1, 2]\n",
      "lif [0, 2, 0, 0]\n",
      "life [0, 1, 0, 0]\n",
      "nam [0, 0, 2, 4]\n",
      "name [0, 0, 1, 2]\n",
      "paul [0, 0, 1, 0]\n",
      "som [0, 8, 0, 0]\n",
      "some [0, 6, 0, 0]\n",
      "someth [0, 4, 0, 0]\n",
      "something [0, 2, 0, 0]\n",
      "wond [2, 0, 0, 0]\n",
      "wonders [1, 0, 0, 0]\n",
      "world [1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "## Calculation of TF\n",
    "i = 0\n",
    "ct = 0\n",
    "tf_line = []\n",
    "doc_counts =[]\n",
    "for term in s:\n",
    "    doc_counts.append(0)\n",
    "    temp = []\n",
    "    for k in range(0,len(tokenize2)) :\n",
    "        line = ()\n",
    "        line = \" \".join(tokenize2[k])\n",
    "        ct = line.count(str(term))\n",
    "        temp.append(ct)\n",
    "        if(ct>0):\n",
    "            doc_counts[i]+=1\n",
    "    tf_line.append(temp)\n",
    "    print(term,tf_line[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF of the text corpus\n",
      "\n",
      "adit [0.0, 0.0, 0.0, 0.23]\n",
      "alright [0.23, 0.0, 0.0, 0.0]\n",
      "boy [0.0, 0.0, 0.201, 0.0]\n",
      "dog [0.0, 0.076, 0.229, 0.0]\n",
      "feel [0.46, 0.0, 0.0, 0.0]\n",
      "feeling [0.23, 0.0, 0.0, 0.0]\n",
      "good [0.0, 0.0, 0.201, 0.0]\n",
      "hello [0.032, 0.019, 0.028, 0.064]\n",
      "lif [0.0, 0.268, 0.0, 0.0]\n",
      "life [0.0, 0.134, 0.0, 0.0]\n",
      "nam [0.0, 0.0, 0.229, 0.524]\n",
      "name [0.0, 0.0, 0.115, 0.262]\n",
      "paul [0.0, 0.0, 0.201, 0.0]\n",
      "som [0.0, 1.073, 0.0, 0.0]\n",
      "some [0.0, 0.805, 0.0, 0.0]\n",
      "someth [0.0, 0.536, 0.0, 0.0]\n",
      "something [0.0, 0.268, 0.0, 0.0]\n",
      "wond [0.46, 0.0, 0.0, 0.0]\n",
      "wonders [0.23, 0.0, 0.0, 0.0]\n",
      "world [0.23, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "## Calculation of TF-IDF in list form\n",
    "tf_idf = []\n",
    "print(\"TF-IDF of the text corpus\\n\")\n",
    "for i in range(0,len(doc_counts)):\n",
    "    idf = []\n",
    "    for j in range(0,len(tf_line[i])):\n",
    "        norm_tf = (tf_line[i][j]/len(tokenize2[j])) \n",
    "        idf_temp = math.log((1+len(tokenize2))/doc_counts[i])\n",
    "        idf.append(float(format(norm_tf*idf_temp,'.3f')))\n",
    "    tf_idf.append(idf)\n",
    "    print(s[i],tf_idf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adit': 1.609, 'alright': 1.609, 'boy': 1.609, 'dog': 0.916, 'feel': 1.609, 'feeling': 1.609, 'good': 1.609, 'hello': 0.223, 'lif': 1.609, 'life': 1.609, 'nam': 0.916, 'name': 0.916, 'paul': 1.609, 'som': 1.609, 'some': 1.609, 'someth': 1.609, 'something': 1.609, 'wond': 1.609, 'wonders': 1.609, 'world': 1.609}\n"
     ]
    }
   ],
   "source": [
    "##Calculation of IDF\n",
    "idf={}\n",
    "for i in range(0,len(s)):\n",
    "    idf[s[i]] = float(format(math.log((1+len(tokenize2))/doc_counts[i]),'.3f'))\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'world': 1, 'wonders': 1, 'hello': 1, 'feeling': 1, 'alright': 1, 'wond': 1, 'feel': 1}, {'hello': 1, 'life': 1, 'something': 2, 'dog': 1, 'lif': 1, 'someth': 2, 'some': 2, 'som': 2}, {'hello': 1, 'dog': 2, 'name': 1, 'paul': 1, 'good': 1, 'boy': 1, 'nam': 1}, {'hello': 2, 'name': 2, 'adit': 1, 'nam': 2}]\n",
      "[{'world': 0.23, 'wonders': 0.258, 'hello': 0.041, 'feeling': 0.355, 'alright': 0.414, 'wond': 0.488, 'feel': 0.578}, {'hello': 0.019, 'life': 0.146, 'something': 0.317, 'dog': 0.108, 'lif': 0.212, 'someth': 0.473, 'some': 0.61, 'som': 0.828}, {'hello': 0.028, 'dog': 0.261, 'name': 0.173, 'paul': 0.361, 'good': 0.421, 'boy': 0.496, 'nam': 0.334}, {'hello': 0.064, 'name': 0.362, 'adit': 0.47, 'nam': 0.633}]\n"
     ]
    }
   ],
   "source": [
    "#Tf-idf in dictionary form for easy calculation\n",
    "print(tokenize3)\n",
    "for i in tokenize3:\n",
    "    for j in i.keys():\n",
    "        for p in idf.keys():\n",
    "            if(j==p):\n",
    "                i[j] = float(format(i[j]/sum(i.values())*idf[p],'.3f'))\n",
    "print(tokenize3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hello Dog Dog\" #Query Input  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'dog', 'dog']\n",
      "{'hello': 0.333, 'dog': 0.667}\n"
     ]
    }
   ],
   "source": [
    "#Calcualtion of normalised tf for query\n",
    "query = query.lower()\n",
    "l = query.split(' ');\n",
    "print(l)\n",
    "l2 = {}\n",
    "for i in l:\n",
    "    if(i not in l2.keys()):\n",
    "        l2[i]=1;\n",
    "    else:\n",
    "        l2[i]+=1\n",
    "for i in l2.keys():\n",
    "    l2[i] = float(format(l2[i]/len(l),'.3f'))\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 0.074, 'dog': 0.611}\n"
     ]
    }
   ],
   "source": [
    "#Calculation of tf-idf for query\n",
    "l3 = {}\n",
    "for i in l2.keys():\n",
    "    if(i in idf.keys()):\n",
    "        l3[i] = float(format(idf[i]*l2[i],'.3f'))\n",
    "print(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.18399761145217622), (2, 0.09294141878128405), (4, 0.03431884057971014), (1, 0.026382608695652173)]\n"
     ]
    }
   ],
   "source": [
    "#Cosine Similarty Calculation\n",
    "cosine_doc = []\n",
    "cosine_doc_dict = {}\n",
    "for i in range(0,len(tokenize3)):\n",
    "    sum1 = 0;\n",
    "    sum2 = 0;\n",
    "    sum3 = 0;\n",
    "    counter = 0\n",
    "    for j in l3.keys():\n",
    "        for k in tokenize3[i].keys():\n",
    "            if(j==k):\n",
    "                sum1 = float(sum1+l3[j]*tokenize3[i][k])\n",
    "                sum2 = float(l3[j]**2+sum2)\n",
    "                sum3 = float(tokenize3[i][k]**2+sum3)\n",
    "                counter = 1; #To check for floating point zero incase nothing of similarity found\n",
    "    if(counter == 1):\n",
    "        cosine_doc.append((sum1/(math.sqrt(sum2)+math.sqrt(sum3))))\n",
    "        cosine_doc_dict[i+1] = cosine_doc[i]\n",
    "    else:\n",
    "        cosine_doc.append(0.0)\n",
    "        cosine_doc_dict[i+1] = cosine_doc[i]\n",
    "##Ranking of document along with the cosine similairty value\n",
    "print(sorted(cosine_doc_dict.items(), key=lambda x: x[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}